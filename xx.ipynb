{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 2.2:* Bandit example Consider a $k$-armed bandit problem with $k = 4$ actions,\n",
    "denoted $1, 2, 3,$ and $4$. Consider applying to this problem a bandit algorithm using\n",
    "$\\epsilon$-greedy action selection, sample-average action-value estimates, and initial estimates\n",
    "of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1 = 1,\n",
    "R_1 = 1, A_2 = 2, R_2 = 1, A_3 = 2, R_3 = 2, A_4 = 2, R_4 = 2, A_5 = 3, R_5 = 0$. On some\n",
    "    of these time steps the $\\epsilon$ case may have occurred, causing an action to be selected at\n",
    "random. On which time steps did this definitely occur? On which time steps could this\n",
    "possibly have occurred?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 2.3* In the comparison shown in *Figure 2.2*, which method will perform best in\n",
    "the long run in terms of cumulative reward and probability of selecting the best action?\n",
    "How much better will it be? Express your answer quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise 2.5 (programming)* Design and conduct an experiment to demonstrate the\n",
    "difficulties that sample-average methods have for nonstationary problems. Use a modified\n",
    "version of the $10-$armed testbed in which all the $q_*(a)$ start out equal and then take\n",
    "independent random walks (say by adding a normally distributed increment with mean\n",
    "zero and standard deviation $0.01$ to all the $q_*(a)$ on each step). Prepare plots like\n",
    "*Figure 2.2* for an action-value method using sample averages, incrementally computed,\n",
    "and another action-value method using a constant step-size parameter, $\\alpha = 0.1$. Use\n",
    "$\\epsilon = 0.1$ and longer runs, say of 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat *Exercise 2.5* with a fixed step size $\\alpha$ but this time use upper confidence bound (UCB) action selection instead of $\\epsilon$-greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat *Exercise 2.5* with a fixed step size $\\alpha$ but this time use the bandit gradient with baseline (see also exercise 2 weeks ago)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General discussion in class on Bernoulli Bandits - Bayes (the beta-distribution), Gittin, linear regression and linear UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
